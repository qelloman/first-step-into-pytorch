{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pytorch Linear Regression Example #  \n",
    "  \n",
    "이번 노트북에서는 전에 배웠던 requires_grad나 backward를 어떻게 사용할 수 있는지  \n",
    "Linear Regression 예제를 이용해 알아볼 예정이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "source": [
    "(위의 import statement는 거의 고정으로 나오니 그냥 외우도록 하자.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 100\n",
    "num_epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.9894],\n        [-1.4234],\n        [-1.3348],\n        [ 6.0159],\n        [-8.8068],\n        [ 4.0964],\n        [ 4.9317],\n        [ 5.4179],\n        [-7.1610],\n        [ 1.2331],\n        [-3.0109],\n        [ 3.4616],\n        [ 8.5172],\n        [-7.4301],\n        [ 5.2134],\n        [ 3.6121],\n        [ 0.8622],\n        [ 4.9028],\n        [-9.4950],\n        [-8.7607],\n        [ 4.7920],\n        [-1.7546],\n        [ 4.3826],\n        [ 4.7746],\n        [ 0.1649],\n        [ 4.5578],\n        [-3.5195],\n        [ 4.5933],\n        [ 8.7658],\n        [ 1.3940],\n        [-5.0402],\n        [ 2.6794],\n        [ 6.4175],\n        [-5.5432],\n        [-7.4574],\n        [-5.5066],\n        [ 4.4444],\n        [-7.8171],\n        [ 1.6071],\n        [ 3.0826],\n        [-9.8816],\n        [ 3.0713],\n        [ 6.8746],\n        [ 3.7746],\n        [-5.8001],\n        [ 6.6887],\n        [ 2.5512],\n        [-5.1637],\n        [ 9.0179],\n        [ 2.1937],\n        [-7.4112],\n        [-6.6269],\n        [ 6.3899],\n        [ 7.9507],\n        [ 7.1052],\n        [ 7.5003],\n        [-6.2047],\n        [ 9.5259],\n        [ 2.5038],\n        [-9.1356],\n        [ 7.2189],\n        [ 3.4120],\n        [ 4.9436],\n        [ 1.5491],\n        [ 3.9864],\n        [ 4.7555],\n        [ 3.2774],\n        [ 9.5242],\n        [-7.4637],\n        [ 9.3233],\n        [ 9.4111],\n        [-8.7755],\n        [-5.8767],\n        [-2.4847],\n        [ 6.1528],\n        [ 0.9309],\n        [-8.5970],\n        [-7.8214],\n        [ 4.0604],\n        [ 3.1152],\n        [ 1.4048],\n        [-5.1527],\n        [ 4.0160],\n        [-7.7306],\n        [-4.6962],\n        [-4.4211],\n        [ 3.9729],\n        [ 8.4551],\n        [ 7.0475],\n        [-0.4904],\n        [ 9.4464],\n        [-4.9657],\n        [ 4.8070],\n        [-5.2596],\n        [ 6.1360],\n        [ 3.8453],\n        [-6.4222],\n        [-4.1724],\n        [ 1.6301],\n        [-0.8683]])\n"
     ]
    }
   ],
   "source": [
    "# tensor를 uniform(-10,10)이라는 distribution에서 나온 샘플들로 채워준 뒤 x로 반환.\n",
    "x = init.uniform_(torch.zeros(num_data,1), -10, 10)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.4075],\n        [-2.5302],\n        [-0.1124],\n        [-0.0932],\n        [-1.3691],\n        [-0.9295],\n        [-0.5958],\n        [-0.1905],\n        [ 2.2968],\n        [-0.3293],\n        [-1.4511],\n        [ 1.5581],\n        [-3.0350],\n        [ 2.4790],\n        [ 1.5015],\n        [ 0.9231],\n        [ 0.2583],\n        [-0.4152],\n        [ 0.8916],\n        [ 1.2110],\n        [ 1.5201],\n        [-0.3739],\n        [ 0.4265],\n        [ 0.1962],\n        [ 0.0585],\n        [-0.4154],\n        [-0.4969],\n        [ 0.7109],\n        [-1.0851],\n        [ 0.5953],\n        [-2.6221],\n        [ 1.9986],\n        [ 0.1849],\n        [-1.8762],\n        [ 0.5513],\n        [-0.2238],\n        [-0.3305],\n        [-2.5283],\n        [-0.1948],\n        [-0.6636],\n        [ 0.3920],\n        [-0.1208],\n        [ 1.8927],\n        [ 1.0064],\n        [ 0.9968],\n        [-0.4645],\n        [ 0.3492],\n        [-0.2334],\n        [-0.5569],\n        [-1.3293],\n        [ 1.3319],\n        [-0.9067],\n        [-1.4152],\n        [-0.8230],\n        [ 0.5644],\n        [-1.7202],\n        [-0.2566],\n        [ 0.1999],\n        [-0.6581],\n        [ 0.7634],\n        [ 0.1000],\n        [ 0.2711],\n        [-0.3104],\n        [-0.1505],\n        [ 0.9241],\n        [-0.9095],\n        [-0.5245],\n        [ 0.1040],\n        [-0.4510],\n        [ 0.7683],\n        [ 0.7561],\n        [-1.5101],\n        [-1.1282],\n        [-0.2777],\n        [ 2.8316],\n        [ 0.2045],\n        [-0.7171],\n        [ 0.3684],\n        [ 2.5787],\n        [-1.4172],\n        [-0.2276],\n        [-0.8054],\n        [ 0.2148],\n        [ 1.4532],\n        [-1.0533],\n        [-0.2272],\n        [-0.4938],\n        [-0.3449],\n        [ 0.5490],\n        [-0.6882],\n        [ 1.4530],\n        [ 0.2691],\n        [-0.0687],\n        [-0.4065],\n        [-0.0491],\n        [-0.2013],\n        [ 0.9746],\n        [-0.3468],\n        [-0.7550],\n        [ 0.8180]])\n"
     ]
    }
   ],
   "source": [
    "# 위와 비슷하게 tensor를 N(0,1) distribution에서 나온 샘플들로 채운 뒤 noise로 반환한다.\n",
    "noise = init.normal_(torch.zeros(num_data,1), std=1)\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2*x + 3 # noise가 없는 y값. 우리의 target이 될 것이다.\n",
    "y_noise = 2*(x+noise) + 3 # noise가 있는 y값. 우리가 실제로 얻은 데이터를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1,1)"
   ]
  },
  {
   "source": [
    "여기서 우리가 만든 model은 Neural Network model이다.  \n",
    "Neural Network의 다양한 모델 중 Linear 모델을 사용하면 1st order linear function을 모델할 수 있다.  \n",
    "nn.Linear 모델은 input과 output의 갯수를 지정하게 된다. (우리는 1,1로 설정했으므로 input도 한개, output도 한개이다.)  \n",
    "Linear 모델은 input과 ouput과의 관계들을 weight로 연결하고, 또한 bias를 가지고 있다.\n",
    "따라서 원래라면 Y = W*X + B 였을 모델이 우리가 input, output이 1개이므로 y = w*x + b가 된다.  \n",
    "Linear Regression이란 결국 Residue의 합이 가장 작아지는 w와 b를 구하는 것이므로  \n",
    "Neural Network 중에 Linear 모델의 **subset**이라 할 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "source": [
    "이 전 예제에서는 우리가 직접 loss 함수를 수식으로 만들어 주었는데  \n",
    "사실은 pytorch에서 다양한 loss 함수를 제공한다.  \n",
    "그 중에서 Lienar Regression에 쓰이는 Mean Square Error를 사용하기로 한다.  \n",
    "이 함수는 나중에 optimizer의 argument로 제공되는데  \n",
    "결국 optimizer한테 \"이 Loss를 최소화 하는 방향으로 진행해줘.\" 라고 전달해 주는 것이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "source": [
    "실제로 학습을 담당하는 객체이다.  \n",
    "SGD는 Stochastic Gradient Descent의 줄임말로 매 샘플마다 parameter를 업데이트하겠다는 말이다.  \n",
    "즉, input X가 500개의 x로 이루어져 있으면 매 x마다 업데이트를 한다. (더 자주 업데이트를 한다는 얘기)  \n",
    "첫번째 arg는 어떤 값들을 업데이트할 것인지에 대한 것인데 우리는 model의 모든 파라미터를 업데이트할 것이다.  \n",
    "즉, 이 optimizer는 loss func를 model의 모든 파라미터들의 편미분한 값을 가지고 모든 파라미터들을 업데이트하게 될 것이다.  \n",
    "lr은 learning rate로 얼마나 빠르게 parameter들을 업데이트할 것인가에 대한 값이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Noise가 없는 이상적인 값(y)을 output label 데이터로 활용했을 때 ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss = 50.891300201416016\nloss = 0.09243965148925781\nloss = 0.001758561935275793\nloss = 3.345064396853559e-05\nloss = 6.363511033669056e-07\n2.00000262260437 2.9998888969421387\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label = y\n",
    "for i in range(num_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # 새로운 데이터들에 대해 다시 기울기를 구하기 위해 초기화.\n",
    "    # 기본적으로 zero_grad는 models.parameters() 안의 weight들의 grad를 0으로 초기화 해준다.\n",
    "    # 그런데 보통 gradient descent technique을 쓰기 위해 쓰이는 batch 연산을 지원하기 위해\n",
    "    # backward() 함수를 호출할 때마다 계산되는 grad값들은 기본적으로 accumulate한다.\n",
    "    # 왜냐하면 모았다가 한꺼번에 계산해서 업데이트하는게 더 빠르기 때문이다.\n",
    "    # weight를 업데이트하고 나서도 grad를 초기화 해주지 않으면 그 전에 사용하던 grad가 그대로 남겨진다.\n",
    "    # 그래서 완전히 새로운 w를 통해 새로 게산되어야 할 grad가 이전 값에 영향을 받게 되므로\n",
    "    # 반드시 zero_grad를 호출해 주어야 한다.\n",
    "\n",
    "    # 차이를 알고 싶다면 밑의 print 구문의 comment를 없애고 실행해보라.\n",
    "    # 참고: https://stackoverflow.com/questions/62067400/understanding-accumulated-gradients-in-pytorch\n",
    "\n",
    "    #param = list(model.parameters())\n",
    "    #print(\"initial grad = {}\".format(param[0].grad))\n",
    "    output = model(x)\n",
    "\n",
    "    loss = loss_func(output, label)\n",
    "    loss.backward()\n",
    "    #print(\"grad after backward() = {}\".format(param[0].grad))\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"loss = {}\".format(loss.data))\n",
    "\n",
    "param_list = list(model.parameters())\n",
    "print(param_list[0].item(), param_list[1].item())"
   ]
  },
  {
   "source": [
    "# 결과 #\n",
    "- 500번의 반복 이후에는 loss가 거의 0에 수렴하고\n",
    "- 원래 w=2, b=3으로 수렴하였다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Noise가 있는 값(y_noise)을 output label 데이터로 활용했을 때 #\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss = 254.05718994140625\n",
      "loss = 4.995692253112793\n",
      "loss = 4.776628494262695\n",
      "loss = 4.772461414337158\n",
      "loss = 4.772382736206055\n",
      "2.0188746452331543 2.898496627807617\n"
     ]
    }
   ],
   "source": [
    "# 위에서 모델 parameter를 다 업데이트 해놨으니 다시 만들어서 초기화\n",
    "model = nn.Linear(1,1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# noisy label\n",
    "label = y_noise\n",
    "for i in range(num_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "\n",
    "    loss = loss_func(output, label)\n",
    "    loss.backward()\n",
    "    #print(\"grad after backward() = {}\".format(param[0].grad))\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"loss = {}\".format(loss.data))\n",
    "\n",
    "param_list = list(model.parameters())\n",
    "print(param_list[0].item(), param_list[1].item())"
   ]
  },
  {
   "source": [
    "# 결과 #\n",
    "- 당연하게도 Noise가 없는 버전보다는 Loss나 최종 w, b가 차이가 있지만\n",
    "- 500번의 반복 이후에는 loss 많이 줄어들었고,\n",
    "- 원래 w=2, b=3으로 수렴하였다.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}